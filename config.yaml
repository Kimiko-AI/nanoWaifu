training:
  batch_size: 128
  learning_rate: 3.0e-4
  num_epochs: 100
  max_train_steps: 1000000
  gradient_checkpointing: true
  freeze_backbone: false
  log_every_steps: 10

  save_image_every_steps: 5000
  image_size: 256
  patch_size: 2   # For 32x32 latents, patch_size 2 gives 16x16=256 patches
  num_workers: 2
  class_dropout_prob: 0.1
  cfg_scale: 4.0
  output_dir: "outputs"
  wandb_project: "nanoWaifu-T2I"
  resume_from: "" # Starting fresh for T2I
  max_checkpoints: 3

model:
  dim: 1536
  depth: 24
  heads: 24
  mlp_dim: 6144
  in_channels: 4 # Latents
  context_dim: 768 # LLM hidden size (adjust based on your LLM)
  vae_model: "stabilityai/sd-vae-ft-mse"
  # LLM text encoder for cross-attention
  text_encoder_model: "openai/clip-vit-large-patch14"  # Replace with your LLM path
  llm_max_seq_length: 512  # Maximum sequence length for LLM

data:
  webdataset_url: "/workspace/shinon/t2i/nanoWaifu/anime/train/000{01..47}.tar"
  csv_path: "characters.csv"
  use_advanced_captions: true  # Enable advanced caption processing with tag dropping

sprint:
  enabled: true
  token_drop_ratio: 0.75  # 0.0 to 0.75 - higher = more aggressive dropping
  encoder_depth: 2        # Number of blocks for encoder (dense shallow)
  middle_depth:  20       # Number of blocks for middle (sparse deep)
  decoder_depth: 2        # Number of blocks for decoder (dense with fusion)
  
  # Two-stage training schedule
  two_stage_training: true
  stage1_steps: 900000    # Long pre-training with token dropping
  stage2_steps: 100000    # Short fine-tuning with full tokens (0% drop)
  
  # Inference optimization
  use_pdg: true          # Path-Drop Guidance for inference (not yet implemented)
