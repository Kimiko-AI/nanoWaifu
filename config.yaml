training:
  batch_size: 16
  learning_rate: 1.0e-4
  num_epochs: 100
  max_train_steps: 1000000
  gradient_checkpointing: true
  freeze_backbone: false
  log_every_steps: 10

  save_image_every_steps: 5000
  image_size: 256
  patch_size: 1   # For 32x32 latents, patch_size 2 gives 16x16=256 patches
  num_workers: 2
  class_dropout_prob: 0
  cfg_scale: 1.4
  output_dir: "outputs"
  wandb_project: "nanoWaifu-T2I"
  resume_from: "" # Starting fresh for T2I
  max_checkpoints: 3

model:
  dim: 1536
  depth: 24
  heads: 16
  mlp_dim: 6144
  in_channels: 128 # Latents
  context_dim: 1024 # LLM hidden size (adjust based on your LLM)
  vae_model: "fal/FLUX.2-Tiny-AutoEncoder"
  # LLM text encoder for cross-attention
  text_encoder_model: "Qwen/Qwen3-0.6B"  # Replace with your LLM path
  llm_max_seq_length: 128  # Maximum sequence length for LLM

data:
  webdataset_url: '/teamspace/studios/this_studio/nanoWaifu-T2I/anime/train/00{001..006}.tar'
  csv_path: "characters.csv"
  use_advanced_captions: true  # Enable advanced caption processing with tag dropping

sprint:
  enabled: true
  token_drop_ratio: 0.75  # 0.0 to 0.75 - higher = more aggressive dropping
  encoder_depth: 2        # Number of blocks for encoder (dense shallow)
  middle_depth:  20       # Number of blocks for middle (sparse deep)
  decoder_depth: 2        # Number of blocks for decoder (dense with fusion)
  
  # Two-stage training schedule
  two_stage_training: true
  stage1_steps: 900000    # Long pre-training with token dropping
  stage2_steps: 100000    # Short fine-tuning with full tokens (0% drop)
  
  # Inference optimization
  use_pdg: true          # Path-Drop Guidance for inference (not yet implemented)
