training:
  batch_size: 128
  learning_rate: 3.0e-4
  num_epochs: 100
  max_train_steps: 1000000
  gradient_checkpointing: true
  freeze_backbone: false
  log_every_steps: 10

  save_image_every_steps: 5000
  image_size: 256  # Small for testing/raw pixel speed
  patch_size: 16   # Small patch size since image is small (64/4 = 16 patches per side)
  num_workers: 2
  class_dropout_prob: 0.1
  cfg_scale: 4.0
  output_dir: "outputs"
  wandb_project: "nanoWaifu-DiT"
  resume_from: "outputs/ckpt_step_65000.pth" # Path to checkpoint to resume from (e.g. "outputs/ckpt_step_1000.pth"), empty for new run
  max_checkpoints: 3 # Keep only top N latest checkpoints

model:
  dim: 1536
  depth: 24 # Number of DiT blocks
  heads: 24
  mlp_dim: 6144 # 4 * dim
  num_classes: 0 # Will be loaded from CSV
  in_channels: 3

data:
  webdataset_url: "/workspace/shinon/t2i/nanoWaifu/anime/train/000{01..47}.tar" # Example URL
  csv_path: "characters.csv"

sprint:
  enabled: true
  token_drop_ratio: 0.75  # 0.0 to 0.75 - higher = more aggressive dropping
  encoder_depth: 8        # Number of blocks for encoder (dense shallow)
  middle_depth:   14       # Number of blocks for middle (sparse deep)
  decoder_depth: 2        # Number of blocks for decoder (dense with fusion)
  
  # Two-stage training schedule
  two_stage_training: true
  stage1_steps: 900000    # Long pre-training with token dropping
  stage2_steps: 100000    # Short fine-tuning with full tokens (0% drop)
  
  # Inference optimization
  use_pdg: true          # Path-Drop Guidance for inference (not yet implemented)
